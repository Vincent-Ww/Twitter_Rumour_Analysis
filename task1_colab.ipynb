{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_project_colab_f (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DaPWyrzHcCCi",
        "q3lGzzmWcCCj",
        "-jSXvSozcCCp",
        "hpney2OYcCCs",
        "fUU9VD4YcCCw",
        "70THatTwcCCx",
        "7yiY2qBWDc99",
        "MQySoyJN0P3J"
      ]
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c3bd62514d448208b80d21a0a4d44f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c4b0f5024b04ebd84d10a07836a7dc5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28e469072be84558b8dd70b7f1bcea13",
              "IPY_MODEL_6b22f16374054640a4352381131c1a14"
            ]
          }
        },
        "5c4b0f5024b04ebd84d10a07836a7dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28e469072be84558b8dd70b7f1bcea13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75a53fd6377e4bc69c32b70f9979ba73",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_310bfe08b58c45e89a0ec50daa4ff754"
          }
        },
        "6b22f16374054640a4352381131c1a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06744300918c4a3ba93ec2d17469fab9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 211kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40e5716f64b34811aba95631af26fb42"
          }
        },
        "75a53fd6377e4bc69c32b70f9979ba73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "310bfe08b58c45e89a0ec50daa4ff754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06744300918c4a3ba93ec2d17469fab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40e5716f64b34811aba95631af26fb42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51cecf60cd1544bba9a509cb65c941f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eeb372e4eedf42fd9fa7972b2ada05d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb19a9dd33fd4e8fbf3876eb9960f39d",
              "IPY_MODEL_f5d0972a2772468cb310f04a9f8608f9"
            ]
          }
        },
        "eeb372e4eedf42fd9fa7972b2ada05d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb19a9dd33fd4e8fbf3876eb9960f39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8327354d22a74826a8149e2a1b892491",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c42bd70a4494f04a19943d9f7c7935f"
          }
        },
        "f5d0972a2772468cb310f04a9f8608f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5927380634404d658f327a8c37f6a2a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 34.8B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1bceaed595143d18c1b239c784732d3"
          }
        },
        "8327354d22a74826a8149e2a1b892491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c42bd70a4494f04a19943d9f7c7935f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5927380634404d658f327a8c37f6a2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1bceaed595143d18c1b239c784732d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c88f69e7a4142d9bfa2590cb2570da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b9301c5c50144f608cca3d251f180033",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_81fc8b8e27af4dd8a57bf89f8ff9db64",
              "IPY_MODEL_c0c813ab3d6e42ada30765291f79175a"
            ]
          }
        },
        "b9301c5c50144f608cca3d251f180033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81fc8b8e27af4dd8a57bf89f8ff9db64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_662329eaa04e442983e03294f42910bd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3db866c38934445686b48a3b546a2142"
          }
        },
        "c0c813ab3d6e42ada30765291f79175a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45cfc3092da540058cdafa883143c9da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 1.37MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cab1ed9efcc64fab87f666c1365282b7"
          }
        },
        "662329eaa04e442983e03294f42910bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3db866c38934445686b48a3b546a2142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45cfc3092da540058cdafa883143c9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cab1ed9efcc64fab87f666c1365282b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fec19e40cd2f44e782a66b90b01713d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_15dab6be997f48468040d9cd8812fa85",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6adfaef5046a436f82de90841623c603",
              "IPY_MODEL_521892d0f54640d99d4f49f62d8bf150"
            ]
          }
        },
        "15dab6be997f48468040d9cd8812fa85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6adfaef5046a436f82de90841623c603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4dcb38aa644242d280f871c3db44680b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf8dc33780d0403bb07fd0659499f069"
          }
        },
        "521892d0f54640d99d4f49f62d8bf150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_581928a99d3c43f7ab32c332fb196620",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 2.26kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d55dfdf4de10471ebe8ffa65cf32d422"
          }
        },
        "4dcb38aa644242d280f871c3db44680b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf8dc33780d0403bb07fd0659499f069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "581928a99d3c43f7ab32c332fb196620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d55dfdf4de10471ebe8ffa65cf32d422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "653b05b264184685b62b7b2a1ef3122b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e1aeddabf424accae7768aba4b3846e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66237b19f48f42e98c22fd55dbd444c1",
              "IPY_MODEL_5889dfb8f14f4a05bd681e1da58608bd"
            ]
          }
        },
        "0e1aeddabf424accae7768aba4b3846e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66237b19f48f42e98c22fd55dbd444c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c5e30900123403aa0035c9bcf3948d4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_333c00b9eb3e493b8cc83a507c7e132b"
          }
        },
        "5889dfb8f14f4a05bd681e1da58608bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_194204aac10648a184195d3d395df030",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:08&lt;00:00, 50.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77ffa593bef84e9fb5ac84879fb45427"
          }
        },
        "9c5e30900123403aa0035c9bcf3948d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "333c00b9eb3e493b8cc83a507c7e132b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "194204aac10648a184195d3d395df030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77ffa593bef84e9fb5ac84879fb45427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHn6ZuCUd4aJ",
        "outputId": "c7ca3d6a-856a-4e0a-ee6a-1cf914955d91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A29zUeJQRb-H",
        "outputId": "679931b7-cd27-4a06-cf29-4fe18edab0ba"
      },
      "source": [
        "import json\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaPWyrzHcCCi"
      },
      "source": [
        "#### reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0CwKZxclQW6"
      },
      "source": [
        "path = \"drive/MyDrive/Colab/NLP_project/\"\n",
        "training_data = [json.loads(event) for event in open(path+'project-data/train.data.jsonl', \"r\").readlines()]\n",
        "dev_data = [json.loads(event) for event in open(path+'project-data/dev.data.jsonl', \"r\").readlines()]\n",
        "test_data = [json.loads(event) for event in open(path+'project-data/test.data.jsonl', \"r\").readlines()]\n",
        "\n",
        "train_labels = json.load(open(path+'project-data/train.label.json', \"r\"))\n",
        "dev_labels = json.load(open(path+'project-data/dev.label.json', \"r\"))\n",
        "\n",
        "train_labels = np.array([(1 if train_labels[id_str]=='rumour' else 0) for id_str in train_labels])\n",
        "dev_labels = np.array([(1 if dev_labels[id_str]=='rumour' else 0) for id_str in dev_labels])\n",
        "\n",
        "test_ids = [test_data[i][0][\"id_str\"] for i in range(len(test_data))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3lGzzmWcCCj"
      },
      "source": [
        "#### preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQv5Pc4gsjFQ"
      },
      "source": [
        "##### sort by date for each event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1x9g7M_suFV"
      },
      "source": [
        "def to_date(date_str):\n",
        "    return datetime.strftime(datetime.strptime(date_str,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "training_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in training_data]\n",
        "dev_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in dev_data]\n",
        "test_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o7z46yEcCCk"
      },
      "source": [
        "##### extract specific attributes(default:text) from twitter dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8EFeU6Pzo3b"
      },
      "source": [
        "def extract_info(data, info=\"text\"):\n",
        "    res = []\n",
        "    for i in range(len(data)):\n",
        "        event = data[i]\n",
        "        event_info = []\n",
        "        for tw in event:\n",
        "            event_info.append(tw[info])\n",
        "        res.append(event_info)\n",
        "    return res\n",
        "\n",
        "training_sents = extract_info(training_data)     # {event}  where event={source,apply1,apply2,...}\n",
        "dev_sents = extract_info(dev_data)\n",
        "test_sents = extract_info(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqq6XGvgcCCl"
      },
      "source": [
        "training_sources0 = [event[0] for event in training_sents]  # [event]  where event={source}\n",
        "dev_sources0 = [event[0] for event in dev_sents]\n",
        "test_sources0 = [event[0] for event in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_czMOfiWP_Bv"
      },
      "source": [
        "##### filter url"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhQe4l9HQDUa"
      },
      "source": [
        "# remove url\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "training_sources00 = [remove_urls(s) for s in training_sources0]\n",
        "dev_sources00 = [remove_urls(s) for s in dev_sources0]\n",
        "test_sources00 = [remove_urls(s) for s in test_sources0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MbAa1c1cCCl"
      },
      "source": [
        "##### word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wkyALDRQJt"
      },
      "source": [
        "training_sources0 = [a+b for a,b in training_all0]\n",
        "dev_sources0 = [a+b for a,b in dev_all0]\n",
        "test_sources0 = [a+b for a,b in test_all0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBiEteFfcCCm"
      },
      "source": [
        "# word tokenization\n",
        "def tokenization(sent):\n",
        "    return [w for w in word_tokenize(sent) if w not in string.punctuation]\n",
        "\n",
        "training_sources1 = [tokenization(s) for s in training_sources0]             \n",
        "dev_sources1 = [tokenization(s) for s in dev_sources0]\n",
        "test_sources1 = [tokenization(s) for s in test_sources0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLCZkNa3cCCm"
      },
      "source": [
        "##### stopword removal + lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX2-eCercCCn"
      },
      "source": [
        "# stopwords removal + lowercase\n",
        "def stop_words_removal(sent):\n",
        "    stop_words = stopwords.words('english')\n",
        "    new_sent = []\n",
        "    for w in sent:\n",
        "        w = w.lower()\n",
        "        if w not in stop_words:\n",
        "            new_sent.append(w)\n",
        "    return new_sent\n",
        "\n",
        "training_sources2 = [stop_words_removal(s) for s in training_sources1]\n",
        "dev_sources2 = [stop_words_removal(s) for s in dev_sources1]\n",
        "test_sources2 = [stop_words_removal(s) for s in test_sources1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNvP8dRWcCCn"
      },
      "source": [
        "##### lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En3cVJcWcCCo"
      },
      "source": [
        "# lemmatization\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def lemmatization(sent):\n",
        "    return [lemmatizer.lemmatize(word) for word in sent]\n",
        "\n",
        "training_sources3 = [lemmatization(s) for s in training_sources2]\n",
        "dev_sources3 = [lemmatization(s) for s in dev_sources2]\n",
        "test_sources3 = [lemmatization(s) for s in test_sources2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBI_AC4LcCCo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N91h61ZcCCp"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jSXvSozcCCp"
      },
      "source": [
        "##### tf-idf + logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uTGQmr7cCCp"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "training_sources4 = [\" \".join(words) for words in training_sources3]\n",
        "dev_sources4 = [\" \".join(words) for words in dev_sources3]\n",
        "test_sources4 = [\" \".join(words) for words in test_sources3]\n",
        "\n",
        "# tf-idf \n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(training_sources4)\n",
        "\n",
        "tfidf_training_sources = tfidf_vectorizer.transform(training_sources4)\n",
        "tfidf_dev_sources = tfidf_vectorizer.transform(dev_sources4)\n",
        "tfidf_test_sources = tfidf_vectorizer.transform(test_sources4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeYMiTJsC_u-"
      },
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "# tf_clf = RandomForestClassifier()\n",
        "# tf_clf.fit(tfidf_training_sources, train_labels)\n",
        "# preds = tf_clf.predict(tfidf_dev_sources)\n",
        "# precision_recall_fscore_support(dev_labels, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3hRIC7FcCCq",
        "outputId": "29cc2374-99ac-4f10-b892-596e0bce4fdc"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(tfidf_training_sources, train_labels)\n",
        "preds = log_reg.predict(tfidf_dev_sources)\n",
        "precision_recall_fscore_support(dev_labels, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.81758242, 0.832     ]),\n",
              " array([0.94656489, 0.55614973]),\n",
              " array([0.87735849, 0.66666667]),\n",
              " array([393, 187]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "831khCiEcCCr",
        "outputId": "96567c6f-94c6-468e-cf5e-e5bb37393e18"
      },
      "source": [
        "# train set\n",
        "preds = log_reg.predict(tfidf_training_sources)\n",
        "preds = [(\"rumour\" if pred==1 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(train_label.keys(), preds))\n",
        "\n",
        "with open(\"preds/lr_train.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P723pV6HcCCr",
        "outputId": "8bcaf900-6f89-4a5e-89b9-d2e6e570c155"
      },
      "source": [
        "# dev set\n",
        "preds = log_reg.predict(tfidf_dev_sources)\n",
        "preds = [(\"rumour\" if pred==1 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(dev_label.keys(), preds))\n",
        "\n",
        "with open(\"preds/lr_dev.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyTybAMlcCCs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpney2OYcCCs"
      },
      "source": [
        "##### tfidf + svc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXVcYYzMcCCt",
        "outputId": "25db2811-f834-4408-d2e8-4ec08cc63944"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "# svc = SVC(C=1.5, tol=1.5, kernel=\"poly\", degree=2)   # 0.85\n",
        "svc = SVC(C=3, tol=1.6)                              # 0.847\n",
        "svc.fit(tfidf_training_sources, train_labels)\n",
        "preds = svc.predict(tfidf_dev_sources)\n",
        "precision_recall_fscore_support(dev_labels, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.87626263, 0.75      ]),\n",
              " array([0.88295165, 0.73796791]),\n",
              " array([0.87959442, 0.74393531]),\n",
              " array([393, 187]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHAorI8zcCCt",
        "outputId": "4af5a69e-4176-4f58-974a-ede852971abc"
      },
      "source": [
        "# on dev set\n",
        "preds = svc.predict(tfidf_dev_sources)\n",
        "preds = [(\"rumour\" if pred==1 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(test_ids, preds))\n",
        "\n",
        "with open(\"svc_dev.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WRH7EQFcCCu",
        "outputId": "8165b3b8-cd8f-4f20-e2ef-aa0b9e49c795"
      },
      "source": [
        "# on training set\n",
        "preds = svc.predict(tfidf_training_sources)\n",
        "preds = [(\"rumour\" if pred==1 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(train_label.keys(), preds))\n",
        "\n",
        "with open(\"preds/svc_train.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDIThZn2cCCu",
        "outputId": "af485834-4892-499f-a5bb-e7df24a155c4"
      },
      "source": [
        "# on test set\n",
        "preds = svc.predict(tfidf_test_sources)\n",
        "preds = [(\"rumour\" if pred==1 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(test_ids, preds))\n",
        "\n",
        "with open(\"svc_test.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xApDpRj6TFyH",
        "outputId": "e504b52c-7771-4c88-8a6a-36bc64369f1e"
      },
      "source": [
        "preds = svc.predict(tfidf_dev_sources)\n",
        "preds = [(1 if pred==1 else 0) for pred in preds]\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision_recall_fscore_support(dev_labels, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.92269327, 0.87150838]),\n",
              " array([0.94147583, 0.8342246 ]),\n",
              " array([0.93198992, 0.85245902]),\n",
              " array([393, 187]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hubN_VZhcCCv"
      },
      "source": [
        "# grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C':[60,55,50,10,5], 'tol':[1e-2,1e-1,1]}\n",
        "svc = SVC()\n",
        "clf = GridSearchCV(svc, param_grid=param_grid,  scoring=\"f1\", n_jobs=-1)\n",
        "clf.fit(tfidf_training_sources, train_labels)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUU9VD4YcCCw"
      },
      "source": [
        "###### cv_results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBNUOXu8cCCw",
        "outputId": "40d575fb-c4d5-4b3a-81d0-0e8fe5957761"
      },
      "source": [
        "print(clf.best_params_)\n",
        "pd.DataFrame(clf.cv_results_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 50, 'tol': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_C</th>\n",
              "      <th>param_tol</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.310851</td>\n",
              "      <td>0.158280</td>\n",
              "      <td>0.930134</td>\n",
              "      <td>0.093922</td>\n",
              "      <td>50</td>\n",
              "      <td>0.001</td>\n",
              "      <td>{'C': 50, 'tol': 0.001}</td>\n",
              "      <td>0.783471</td>\n",
              "      <td>0.792321</td>\n",
              "      <td>0.784854</td>\n",
              "      <td>0.783362</td>\n",
              "      <td>0.775221</td>\n",
              "      <td>0.783846</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.741047</td>\n",
              "      <td>0.104816</td>\n",
              "      <td>0.953380</td>\n",
              "      <td>0.048291</td>\n",
              "      <td>50</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 50, 'tol': 0.1}</td>\n",
              "      <td>0.785479</td>\n",
              "      <td>0.794425</td>\n",
              "      <td>0.788296</td>\n",
              "      <td>0.785467</td>\n",
              "      <td>0.777385</td>\n",
              "      <td>0.786210</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.000468</td>\n",
              "      <td>0.109522</td>\n",
              "      <td>0.565687</td>\n",
              "      <td>0.028922</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 50, 'tol': 1}</td>\n",
              "      <td>0.802528</td>\n",
              "      <td>0.794658</td>\n",
              "      <td>0.787479</td>\n",
              "      <td>0.806612</td>\n",
              "      <td>0.782462</td>\n",
              "      <td>0.794748</td>\n",
              "      <td>0.009001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.084618</td>\n",
              "      <td>0.156478</td>\n",
              "      <td>0.946514</td>\n",
              "      <td>0.046767</td>\n",
              "      <td>10</td>\n",
              "      <td>0.001</td>\n",
              "      <td>{'C': 10, 'tol': 0.001}</td>\n",
              "      <td>0.783471</td>\n",
              "      <td>0.792321</td>\n",
              "      <td>0.784854</td>\n",
              "      <td>0.783362</td>\n",
              "      <td>0.775221</td>\n",
              "      <td>0.783846</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.707623</td>\n",
              "      <td>0.146584</td>\n",
              "      <td>0.903296</td>\n",
              "      <td>0.086834</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 10, 'tol': 0.1}</td>\n",
              "      <td>0.785479</td>\n",
              "      <td>0.794425</td>\n",
              "      <td>0.788296</td>\n",
              "      <td>0.785467</td>\n",
              "      <td>0.777385</td>\n",
              "      <td>0.786210</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.092335</td>\n",
              "      <td>0.100873</td>\n",
              "      <td>0.524120</td>\n",
              "      <td>0.070022</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 10, 'tol': 1}</td>\n",
              "      <td>0.802528</td>\n",
              "      <td>0.794658</td>\n",
              "      <td>0.787479</td>\n",
              "      <td>0.806612</td>\n",
              "      <td>0.782462</td>\n",
              "      <td>0.794748</td>\n",
              "      <td>0.009001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.184953</td>\n",
              "      <td>0.330972</td>\n",
              "      <td>0.956286</td>\n",
              "      <td>0.162735</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001</td>\n",
              "      <td>{'C': 5, 'tol': 0.001}</td>\n",
              "      <td>0.783471</td>\n",
              "      <td>0.792321</td>\n",
              "      <td>0.784854</td>\n",
              "      <td>0.783362</td>\n",
              "      <td>0.775221</td>\n",
              "      <td>0.783846</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.298766</td>\n",
              "      <td>0.408519</td>\n",
              "      <td>0.841960</td>\n",
              "      <td>0.105925</td>\n",
              "      <td>5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 5, 'tol': 0.1}</td>\n",
              "      <td>0.785479</td>\n",
              "      <td>0.794425</td>\n",
              "      <td>0.786207</td>\n",
              "      <td>0.785467</td>\n",
              "      <td>0.777385</td>\n",
              "      <td>0.785793</td>\n",
              "      <td>0.005396</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.981279</td>\n",
              "      <td>0.180946</td>\n",
              "      <td>0.527091</td>\n",
              "      <td>0.042000</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 5, 'tol': 1}</td>\n",
              "      <td>0.802528</td>\n",
              "      <td>0.794658</td>\n",
              "      <td>0.787479</td>\n",
              "      <td>0.806612</td>\n",
              "      <td>0.782462</td>\n",
              "      <td>0.794748</td>\n",
              "      <td>0.009001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.825034</td>\n",
              "      <td>0.213924</td>\n",
              "      <td>0.902639</td>\n",
              "      <td>0.081434</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>{'C': 1, 'tol': 0.001}</td>\n",
              "      <td>0.769748</td>\n",
              "      <td>0.754850</td>\n",
              "      <td>0.748663</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.753153</td>\n",
              "      <td>0.754302</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.508998</td>\n",
              "      <td>0.163617</td>\n",
              "      <td>0.832798</td>\n",
              "      <td>0.061902</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 1, 'tol': 0.1}</td>\n",
              "      <td>0.768456</td>\n",
              "      <td>0.756184</td>\n",
              "      <td>0.748663</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.751799</td>\n",
              "      <td>0.753592</td>\n",
              "      <td>0.008607</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.917327</td>\n",
              "      <td>0.149687</td>\n",
              "      <td>0.499464</td>\n",
              "      <td>0.053990</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 1, 'tol': 1}</td>\n",
              "      <td>0.787781</td>\n",
              "      <td>0.776271</td>\n",
              "      <td>0.765812</td>\n",
              "      <td>0.789298</td>\n",
              "      <td>0.770035</td>\n",
              "      <td>0.777839</td>\n",
              "      <td>0.009361</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.129145</td>\n",
              "      <td>0.357086</td>\n",
              "      <td>0.813437</td>\n",
              "      <td>0.102749</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.001</td>\n",
              "      <td>{'C': 0.01, 'tol': 0.001}</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2.761084</td>\n",
              "      <td>0.205051</td>\n",
              "      <td>0.698592</td>\n",
              "      <td>0.081178</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 0.01, 'tol': 0.1}</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2.205191</td>\n",
              "      <td>0.203854</td>\n",
              "      <td>0.456201</td>\n",
              "      <td>0.044341</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 0.01, 'tol': 1}</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
              "0        4.310851      0.158280         0.930134        0.093922      50   \n",
              "1        3.741047      0.104816         0.953380        0.048291      50   \n",
              "2        2.000468      0.109522         0.565687        0.028922      50   \n",
              "3        4.084618      0.156478         0.946514        0.046767      10   \n",
              "4        3.707623      0.146584         0.903296        0.086834      10   \n",
              "5        2.092335      0.100873         0.524120        0.070022      10   \n",
              "6        4.184953      0.330972         0.956286        0.162735       5   \n",
              "7        3.298766      0.408519         0.841960        0.105925       5   \n",
              "8        1.981279      0.180946         0.527091        0.042000       5   \n",
              "9        3.825034      0.213924         0.902639        0.081434       1   \n",
              "10       3.508998      0.163617         0.832798        0.061902       1   \n",
              "11       1.917327      0.149687         0.499464        0.053990       1   \n",
              "12       3.129145      0.357086         0.813437        0.102749    0.01   \n",
              "13       2.761084      0.205051         0.698592        0.081178    0.01   \n",
              "14       2.205191      0.203854         0.456201        0.044341    0.01   \n",
              "\n",
              "   param_tol                     params  split0_test_score  split1_test_score  \\\n",
              "0      0.001    {'C': 50, 'tol': 0.001}           0.783471           0.792321   \n",
              "1        0.1      {'C': 50, 'tol': 0.1}           0.785479           0.794425   \n",
              "2          1        {'C': 50, 'tol': 1}           0.802528           0.794658   \n",
              "3      0.001    {'C': 10, 'tol': 0.001}           0.783471           0.792321   \n",
              "4        0.1      {'C': 10, 'tol': 0.1}           0.785479           0.794425   \n",
              "5          1        {'C': 10, 'tol': 1}           0.802528           0.794658   \n",
              "6      0.001     {'C': 5, 'tol': 0.001}           0.783471           0.792321   \n",
              "7        0.1       {'C': 5, 'tol': 0.1}           0.785479           0.794425   \n",
              "8          1         {'C': 5, 'tol': 1}           0.802528           0.794658   \n",
              "9      0.001     {'C': 1, 'tol': 0.001}           0.769748           0.754850   \n",
              "10       0.1       {'C': 1, 'tol': 0.1}           0.768456           0.756184   \n",
              "11         1         {'C': 1, 'tol': 1}           0.787781           0.776271   \n",
              "12     0.001  {'C': 0.01, 'tol': 0.001}           0.000000           0.000000   \n",
              "13       0.1    {'C': 0.01, 'tol': 0.1}           0.000000           0.000000   \n",
              "14         1      {'C': 0.01, 'tol': 1}           0.000000           0.000000   \n",
              "\n",
              "    split2_test_score  split3_test_score  split4_test_score  mean_test_score  \\\n",
              "0            0.784854           0.783362           0.775221         0.783846   \n",
              "1            0.788296           0.785467           0.777385         0.786210   \n",
              "2            0.787479           0.806612           0.782462         0.794748   \n",
              "3            0.784854           0.783362           0.775221         0.783846   \n",
              "4            0.788296           0.785467           0.777385         0.786210   \n",
              "5            0.787479           0.806612           0.782462         0.794748   \n",
              "6            0.784854           0.783362           0.775221         0.783846   \n",
              "7            0.786207           0.785467           0.777385         0.785793   \n",
              "8            0.787479           0.806612           0.782462         0.794748   \n",
              "9            0.748663           0.745098           0.753153         0.754302   \n",
              "10           0.748663           0.742857           0.751799         0.753592   \n",
              "11           0.765812           0.789298           0.770035         0.777839   \n",
              "12           0.000000           0.000000           0.000000         0.000000   \n",
              "13           0.000000           0.000000           0.000000         0.000000   \n",
              "14           0.000000           0.000000           0.000000         0.000000   \n",
              "\n",
              "    std_test_score  rank_test_score  \n",
              "0         0.005433                7  \n",
              "1         0.005492                4  \n",
              "2         0.009001                1  \n",
              "3         0.005433                7  \n",
              "4         0.005492                4  \n",
              "5         0.009001                1  \n",
              "6         0.005433                7  \n",
              "7         0.005396                6  \n",
              "8         0.009001                1  \n",
              "9         0.008446               11  \n",
              "10        0.008607               12  \n",
              "11        0.009361               10  \n",
              "12        0.000000               13  \n",
              "13        0.000000               13  \n",
              "14        0.000000               13  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzyQjQ8DcCCw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8bXa13KcCCx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70THatTwcCCx"
      },
      "source": [
        "##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG7s5OeccCCx"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM\n",
        "from keras import metrics\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXj6IAiLcCCy"
      },
      "source": [
        "# tokenization to sequences (order is perserved)\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(training_sources0)\n",
        "\n",
        "training_sources4 = tokenizer.texts_to_sequences(training_sources0)\n",
        "dev_sources4 = tokenizer.texts_to_sequences(dev_sources0)\n",
        "test_sources4 = tokenizer.texts_to_sequences(test_sources0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJp4_lAAcCCy"
      },
      "source": [
        "# lengths = [len(s) for s in training_sources4]\n",
        "# print(max(lengths))\n",
        "# print(min(lengths))\n",
        "# pd.Series(lengths).hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQS4iVicCCz"
      },
      "source": [
        "# padding => each sent has the same length\n",
        "maxlen = 25\n",
        "training_sources5 = pad_sequences(training_sources4, padding=\"post\", maxlen=maxlen)\n",
        "dev_sources5 = pad_sequences(dev_sources4, padding=\"post\", maxlen=maxlen)\n",
        "test_sources5 = pad_sequences(test_sources4, padding=\"post\", maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm_4pphprT-3",
        "outputId": "c7cb27e3-6f77-4382-c0e1-032ffcc37bac"
      },
      "source": [
        "training_sources5.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o6iga_2cCCz"
      },
      "source": [
        "vocab_size = len(tokenizer.word_counts)+2\n",
        "embedding_dim = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZNz4waScCCz"
      },
      "source": [
        "#taken from old keras source code\n",
        "def get_f1(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwAhEPMCcCC0"
      },
      "source": [
        "lstm_nn = Sequential(name=\"lstm\")\n",
        "lstm_nn.add(layers.Embedding(input_dim = vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=maxlen))\n",
        "lstm_nn.add(LSTM(128))\n",
        "lstm_nn.add(layers.Dense(128, activation=\"relu\", name=\"FC1\"))\n",
        "lstm_nn.add(layers.Dropout(0.5))\n",
        "lstm_nn.add(layers.Dense(1, activation='sigmoid', name=\"FC2\"))\n",
        "lstm_nn.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=[metrics.AUC(), \"acc\"],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO8EnPjjcCC0",
        "outputId": "ee44a051-646e-4aee-d8ba-ab1721ecc32f"
      },
      "source": [
        "lstm_nn.fit(training_sources5, \n",
        "            train_labels, \n",
        "            shuffle=True,\n",
        "            epochs=10, \n",
        "            verbose=True, \n",
        "            validation_data=(dev_sources5, dev_labels),\n",
        "            batch_size = 128,\n",
        "            callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.01)] \n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "37/37 [==============================] - 3s 34ms/step - loss: 0.6454 - auc_8: 0.5593 - acc: 0.6317 - val_loss: 0.3645 - val_auc_8: 0.9171 - val_acc: 0.8552\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.3102 - auc_8: 0.9362 - acc: 0.8782 - val_loss: 0.3284 - val_auc_8: 0.9263 - val_acc: 0.8724\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - 1s 21ms/step - loss: 0.1383 - auc_8: 0.9866 - acc: 0.9493 - val_loss: 0.3691 - val_auc_8: 0.9229 - val_acc: 0.8724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3993046650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW-wTWsewBmR",
        "outputId": "faced6cc-ec38-42d4-fba7-aafb9fd35045"
      },
      "source": [
        "preds = lstm_nn.predict(test_sources5)\n",
        "preds = [(\"rumour\" if pred>0.5 else \"non-rumour\") for pred in preds]\n",
        "preds_dict = dict(zip(test_ids, preds))\n",
        "\n",
        "with open(\"sample_data/lstm_test.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B277yKbxisD",
        "outputId": "24578a10-08db-4079-b394-7cf1c2786187"
      },
      "source": [
        "lstm_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"lstm\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 25, 128)           1443072   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "FC2 (Dense)                  (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,591,297\n",
            "Trainable params: 1,591,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRwcS4G69ckj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yiY2qBWDc99"
      },
      "source": [
        "##### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkioaw3DnE9c",
        "outputId": "29b8c92b-c464-4c7b-a823-d3621aa9a32e"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 42.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQzlwfzGS8oR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV6j7yfRDd6L"
      },
      "source": [
        "# define the dataset class\n",
        "class TwitrerDataset(Dataset):\n",
        "  def __init__(self, X, y, maxlen):\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.maxlen = maxlen\n",
        "    self.X = X\n",
        "    self.y = y \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # selecting the sentence and label at the specific index\n",
        "    sent = self.X[index]\n",
        "    label = self.y[index]\n",
        "\n",
        "    # preprocessing the text to be suitable for BERT\n",
        "    tokens = self.tokenizer.tokenize(sent)\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP']       # insert CLS and SEP token\n",
        "    if len(tokens) < self.maxlen:                # keep the same length of each sentence\n",
        "      tokens = tokens + ['[PAD]' for _ in range(self.maxlen-len(tokens))]\n",
        "    else:\n",
        "      tokens = tokens[:self.maxlen-1] + ['SEP']\n",
        "\n",
        "    tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) # obtaining the indices of tokens in vocab\n",
        "    tokens_ids_tensor = torch.tensor(tokens_ids)   \n",
        "\n",
        "    attn_mask = (tokens_ids_tensor != 0).long()        # attention mask (identity where is padded)\n",
        "    \n",
        "    return tokens_ids_tensor, attn_mask, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4rCqdOu9hcb"
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "num_worders = 2\n",
        "lr = 2e-5\n",
        "maxlen = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ620-MOdLP9",
        "outputId": "99bf4547-83b7-4e4c-f509-e006bfde0096"
      },
      "source": [
        "# creating instances of training and dev set\n",
        "train_set = TwitrerDataset(training_sources0, train_labels, maxlen=maxlen)\n",
        "dev_set = TwitrerDataset(dev_sources0, dev_labels, maxlen=maxlen)\n",
        "\n",
        "# creating dataset loader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_worders)\n",
        "dev_loader = DataLoader(dev_set, batch_size=batch_size, num_workers=num_worders)\n",
        "\n",
        "print(\"Done preprocessing training and development data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done preprocessing training and development data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_v7akQ_-DFz"
      },
      "source": [
        "class RumourClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(RumourClassifier, self).__init__()\n",
        "    self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "  def forward(self, seq, attn_masks):\n",
        "    '''\n",
        "    Inputs:\n",
        "      -seq: Tensor of shape [B, T] containing token ids of sequences\n",
        "      -attn_masks: Tensor of shape [B, T] containing attention masks to be used\n",
        "    '''\n",
        "\n",
        "    # feed the input to bert model to obtain contextualized representation\n",
        "    outputs = self.bert_layer(seq, attention_mask=attn_masks)\n",
        "    cont_reps = outputs.last_hidden_state\n",
        "\n",
        "    # obtaining the representation of [CLS] head\n",
        "    cls_rep = cont_reps[:, 0]\n",
        "\n",
        "    # feeding cls_rep into the classifier layer\n",
        "    logits = self.cls_layer(cls_rep)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evO8tW60ndwb"
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def get_precision_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    # de = 0\n",
        "    # nu = 0\n",
        "    # for i in range(len(soft_probs)):\n",
        "    #     if soft_probs[i] == 1:\n",
        "    #         de += 1\n",
        "    #         if labels[i] == 1:\n",
        "    #           nu += 1\n",
        "    # return nu / de\n",
        "    labels_cpu = labels.cpu()\n",
        "    soft_probs_cpu = soft_probs.cpu()\n",
        "    return precision_recall_fscore_support(labels_cpu, soft_probs_cpu)[0][1]\n",
        "\n",
        "\n",
        "def get_recall_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    # de = 0\n",
        "    # nu = 0\n",
        "    # for i in range(len(labels)):\n",
        "    #     if labels[i] == 1:\n",
        "    #         de += 1\n",
        "    #         if soft_probs[i] == 1:\n",
        "    #           nu += 1\n",
        "    # return nu / de\n",
        "    labels_cpu = labels.cpu()\n",
        "    soft_probs_cpu = soft_probs.cpu()\n",
        "    return precision_recall_fscore_support(labels_cpu, soft_probs_cpu)[1][1]\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss, mean_precision, mean_recall = 0, 0, 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            bs = labels.shape[0]\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()*bs\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)*bs\n",
        "            mean_precision += get_precision_from_logits(logits, labels)*bs\n",
        "            mean_recall += get_recall_from_logits(logits, labels)*bs\n",
        "            count += bs\n",
        "\n",
        "    return mean_acc / count, mean_precision / count, mean_recall / count,mean_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmSh13geHTVv"
      },
      "source": [
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 100 == 0:\n",
        "                \n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                precision = get_precision_from_logits(logits, labels)\n",
        "                recall = get_recall_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Precision: {}; Recall: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, precision, recall,(time.time()-st)))\n",
        "                st = time.time()\n",
        "\n",
        "        \n",
        "        dev_acc, dev_precision, dev_recall,dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Precision: {}; Development Recall: {}; Development Loss: {}\".format(ep, dev_acc, dev_precision, dev_recall, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkvZk6GKCJ-k",
        "outputId": "8fd84864-adb7-4731-b68c-d6afa3e3ed95"
      },
      "source": [
        "gpu = 0  # GPU id\n",
        "\n",
        "bert_net = RumourClassifier()\n",
        "bert_net.cuda(gpu) \n",
        "print(\"creating the rumour classifier: Done!\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating the rumour classifier: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDwJ-SfMH4If",
        "outputId": "267c08e5-1736-468b-f414-ba51582204c6"
      },
      "source": [
        "num_epoch = 1\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(bert_net.parameters(), lr=lr)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 of epoch 0 complete. Loss: 0.7238537073135376; Accuracy: 0.40625; Precision: 0.3333333333333333; Recall: 0.7272727272727273; Time taken (s): 0.4756500720977783\n",
            "Iteration 100 of epoch 0 complete. Loss: 0.432309091091156; Accuracy: 0.875; Precision: 1.0; Recall: 0.6923076923076923; Time taken (s): 18.85442590713501\n",
            "Epoch 0 complete! Development Accuracy: 0.8379310369491577; Development Precision: 0.6819441055012743; Development Recall: 0.9149200225062293; Development Loss: 0.35003519798147265\n",
            "Best development accuracy improved from 0 to 0.8379310369491577, saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h60E6HTkBtM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwLgMtvqBtQT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB76MtHJItwJ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fd81E3xPDbH"
      },
      "source": [
        "def predict(sent, maxlen=maxlen):\n",
        "  tokens = tokenizer.tokenize(sent)\n",
        "  tokens = tokens = ['[CLS]'] + tokens + ['[SEP']\n",
        "  if len(tokens) < maxlen:                # keep the same length of each sentence\n",
        "    tokens = tokens + ['[PAD]' for _ in range(maxlen-len(tokens))]\n",
        "  else:\n",
        "    tokens = tokens[:maxlen-1] + ['SEP']\n",
        "  tokens_ids = tokenizer.convert_tokens_to_ids(tokens) # obtaining the indices of tokens in vocab\n",
        "  tokens_ids_tensor = torch.tensor(tokens_ids).unsqueeze(0).cuda(gpu)\n",
        "\n",
        "  attn_mask = (tokens_ids_tensor != 0).long().cuda(gpu)        # attention mask (identity where is padded)\n",
        "  with torch.no_grad():\n",
        "    prediction = bert_net(tokens_ids_tensor, attn_mask)\n",
        "  return prediction"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTZ9hn0LQezM"
      },
      "source": [
        "predictions = [predict(sent) for sent in test_sources0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKErxCruUxl0"
      },
      "source": [
        "preds1 = [(\"rumour\" if pred > 0.0 else \"non-rumour\") for pred in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-FcOycEYk30",
        "outputId": "1cbe8425-cef0-496f-9c10-9fd558c07d23"
      },
      "source": [
        "preds_dict = dict(zip(test_ids, preds1))\n",
        "\n",
        "with open(\"sample_data/bert.v2_test_epoch10.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bRk7IR7DPsF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x9FJkxaWD5C",
        "outputId": "f262d5b9-3952-4d57-f4fa-22e986a1eeb9"
      },
      "source": [
        "preds_dev = [(1 if p >0.0 else 0) for p in [predict(sent) for sent in dev_sources0]]\n",
        "precision_recall_fscore_support(preds_dev, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.80152672, 0.9144385 ]),\n",
              " array([0.95166163, 0.68674699]),\n",
              " array([0.87016575, 0.78440367]),\n",
              " array([331, 249]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_6aRUST2JSi",
        "outputId": "ef765e43-05e9-48ce-88cb-6194869433bf"
      },
      "source": [
        "get_recall_from_logits(torch.tensor(preds_dev), torch.tensor(dev_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8074866310160428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC2vnU4X21zu",
        "outputId": "8567b429-fc06-4cfa-8ea5-839cabb1f013"
      },
      "source": [
        "count = 0\n",
        "mean_acc, mean_precision, mean_recall = 0,0,0\n",
        "for seq, attn_masks, labels in dev_loader:\n",
        "    bs = (labels.shape[0])\n",
        "    seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "    logits = bert_net(seq, attn_masks)\n",
        "    mean_acc += get_accuracy_from_logits(logits, labels) * bs\n",
        "    mean_precision += get_precision_from_logits(logits, labels) * bs\n",
        "    mean_recall += get_recall_from_logits(logits, labels) * bs\n",
        "    count += bs\n",
        "print(mean_precision / count)\n",
        "print(mean_recall / count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.759852943990875\n",
            "0.8160694477935857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3el6ZljqOXl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cO3u5N8ef3x",
        "outputId": "60934cd0-ca3f-448c-9962-2bf29943b1b1"
      },
      "source": [
        "preds_train = [(1 if p >0.0 else 0) for p in [predict(sent) for sent in training_sources0]]\n",
        "precision_recall_fscore_support(preds_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.99444081, 0.99747315]),\n",
              " array([0.99868637, 0.98934837]),\n",
              " array([0.99655907, 0.99339415]),\n",
              " array([3045, 1596]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQKzUctP0Og1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQySoyJN0P3J"
      },
      "source": [
        "##### Bert.v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwWKxDBcPB3q"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcnTQKCX0RxX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybFbUtkNT8ig"
      },
      "source": [
        "def to_date(date_str):\n",
        "    return datetime.strftime(datetime.strptime(date_str,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "training_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in training_data]\n",
        "dev_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in dev_data]\n",
        "test_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT_kxnBrUAjs"
      },
      "source": [
        "# remove url\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def extract_info(data, info=\"text\"):\n",
        "    res = []\n",
        "    for i in range(len(data)):\n",
        "        event = data[i]\n",
        "        event_info = []\n",
        "        for tw in event:\n",
        "            event_info.append(remove_urls(tw[info]))\n",
        "        res.append(event_info)\n",
        "    return res\n",
        "\n",
        "training_sents = extract_info(training_data)     # {event}  where event={source,apply1,apply2,...}\n",
        "dev_sents = extract_info(dev_data)\n",
        "test_sents = extract_info(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ube9CJX4UEe3"
      },
      "source": [
        "def combine_replies(replies):\n",
        "    res = \"\"\n",
        "    for r in replies:\n",
        "        res += r\n",
        "    return res\n",
        "\n",
        "training_all0 = [[event[0], combine_replies(event[1:])] for event in training_sents]\n",
        "dev_all0 = [[event[0], combine_replies(event[1:])] for event in dev_sents]\n",
        "test_all0 = [[event[0], combine_replies(event[1:])] for event in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQw869FhO79r"
      },
      "source": [
        "# define the dataset class\n",
        "class TwitrerDataset4(Dataset):\n",
        "  def __init__(self, X, y, source_maxlen, reply_maxlen):\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.source_maxlen = source_maxlen\n",
        "    self.reply_maxlen = reply_maxlen\n",
        "    self.X = X\n",
        "    self.y = y \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # selecting the sentence and label at the specific index\n",
        "    # sent = self.X[index]\n",
        "    source, replies = self.X[index]\n",
        "    label = self.y[index]\n",
        "\n",
        "    # preprocessing the text to be suitable for BERT\n",
        "    s_tokens = self.tokenizer.tokenize(source)\n",
        "    s_tokens = ['[CLS]'] + s_tokens + ['[SEP']       # insert CLS and SEP token\n",
        "    if len(s_tokens) < self.source_maxlen:                # keep the same length of each sentence\n",
        "      s_tokens = s_tokens + ['[PAD]' for _ in range(self.source_maxlen-len(s_tokens))]\n",
        "    else:\n",
        "      s_tokens = s_tokens[:self.source_maxlen-1] + ['SEP']\n",
        "\n",
        "    r_tokens = self.tokenizer.tokenize(replies)\n",
        "    r_tokens = r_tokens + ['[SEP']\n",
        "    if len(r_tokens) < self.reply_maxlen:                # keep the same length of each sentence\n",
        "      r_tokens = r_tokens + ['[PAD]' for _ in range(self.reply_maxlen-len(r_tokens))]\n",
        "    else:\n",
        "      r_tokens = r_tokens[:self.reply_maxlen-1] + ['SEP']\n",
        "\n",
        "\n",
        "    tokens_ids = self.tokenizer.convert_tokens_to_ids(s_tokens) + self.tokenizer.convert_tokens_to_ids(r_tokens) # obtaining the indices of tokens in vocab\n",
        "    tokens_ids_tensor = torch.tensor(tokens_ids)   \n",
        "\n",
        "    attn_mask = (tokens_ids_tensor != 0).long()        # attention mask (identity where is padded)\n",
        "\n",
        "    token_type_ids = torch.tensor([0 for _ in range(source_maxlen)]+[1 for _ in range(reply_maxlen)])\n",
        "    \n",
        "    return tokens_ids_tensor, attn_mask, token_type_ids, label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEJfd0qCUfnS",
        "outputId": "751361ed-bb3a-4876-c486-60983a1a46c4"
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 128\n",
        "num_worders = 2\n",
        "lr = 2e-5\n",
        "source_maxlen, reply_maxlen = 30, 30\n",
        "\n",
        "# creating instances of training and dev set\n",
        "train_set = TwitrerDataset4(training_all0, train_labels, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen)\n",
        "dev_set = TwitrerDataset4(dev_all0, dev_labels, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen)\n",
        "\n",
        "# creating dataset loader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_worders)\n",
        "dev_loader = DataLoader(dev_set, batch_size=batch_size, num_workers=num_worders)\n",
        "\n",
        "print(\"Done preprocessing training and development data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done preprocessing training and development data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsaDBJHMPbcO"
      },
      "source": [
        "class RumourClassifier4(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(RumourClassifier4, self).__init__()\n",
        "    self.bert_layer = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "  def forward(self, seq, attn_masks, token_type_ids):\n",
        "    '''\n",
        "    Inputs:\n",
        "      -seq: Tensor of shape [B, T] containing token ids of sequences\n",
        "      -attn_masks: Tensor of shape [B, T] containing attention masks to be used\n",
        "    '''\n",
        "    # feed the input to bert model to obtain contextualized representation\n",
        "    outputs = self.bert_layer(seq, attention_mask=attn_masks, token_type_ids=token_type_ids)\n",
        "    logits = outputs.logits\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENiI2p8h2U65"
      },
      "source": [
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), token_type_ids.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.view(-1,2), labels.long().view(-1))\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 10 == 0:\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {};  Time taken (s): {}\".format(it, ep, loss.item(), (time.time()-st)))\n",
        "                st = time.time()\n",
        "\n",
        "        \n",
        "        dev_precision, dev_recall,dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        dev_f1 = (2*dev_precision*dev_recall/(dev_precision+dev_recall))\n",
        "        print(\"*****Epoch {} complete! Development f1-score: {}; Development Precision: {}; Development Recall: {}; Development Loss: {}\".format(ep, dev_f1, dev_precision, dev_recall, dev_loss))\n",
        "        if dev_f1 > best_f1:\n",
        "            print(\"Best development f1-score improved from {} to {}, saving model...\".format(best_f1, dev_f1))\n",
        "            best_f1 = dev_f1\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szisViem2xNX"
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).long().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss, mean_precision, mean_recall = 0, 0, 0, 0\n",
        "    count = 0\n",
        "    tn, fp, fn, tp = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, token_type_ids, labels in dataloader:\n",
        "            bs = labels.shape[0]\n",
        "            seq, attn_masks, token_type_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), token_type_ids.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.long()).item()*bs\n",
        "            # mean_acc += get_accuracy_from_logits(logits, labels)*bs\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            # soft_probs = (probs > 0.5).long()\n",
        "            soft_probs = [(1 if a < b else 0) for a,b in probs]\n",
        "            labels_cpu, soft_probs_cpu = labels.cpu(), soft_probs.cpu()\n",
        "            tn_, fp_, fn_, tp_ = confusion_matrix(labels_cpu, soft_probs_cpu).ravel()\n",
        "            tn += tn_\n",
        "            fp += fp_\n",
        "            fn += fn_\n",
        "            tp += tp_\n",
        "            count += bs\n",
        "    return  tp / (tp+fp), tp / (tp+fn), mean_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gdfgBJM3XTb",
        "outputId": "8f86803b-77aa-476f-f05a-17da96b80c77"
      },
      "source": [
        "gpu = 0  # GPU id\n",
        "\n",
        "bert_net4 = RumourClassifier4()\n",
        "bert_net4.cuda(gpu) \n",
        "print(\"creating the rumour classifier4: Done!\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "creating the rumour classifier4: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "P_Zx_k7x264E",
        "outputId": "4f7591a0-ea74-4ca0-9cb8-22eb9a039444"
      },
      "source": [
        "num_epoch = 1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opti = optim.Adam(bert_net4.parameters(), lr=lr)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net4, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-70066319335c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fine-tune the bert network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_net4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-e357716f2633>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, criterion, opti, train_loader, dev_loader, max_eps, gpu)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m#Obtaining the logits from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;31m#Computing loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-75490ccba228>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq, attn_masks, token_type_ids)\u001b[0m\n\u001b[1;32m     13\u001b[0m     '''\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# feed the input to bert model to obtain contextualized representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m         )\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    463\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         )\n\u001b[1;32m    396\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 13.20 GiB already allocated; 13.75 MiB free; 13.71 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzcJrOXZ3GY-"
      },
      "source": [
        "# def predict2(sent, maxlen=25):\n",
        "#   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#   tokens = tokenizer.tokenize(sent)\n",
        "#   tokens = tokens = ['[CLS]'] + tokens + ['[SEP']\n",
        "#   if len(tokens) < maxlen:                # keep the same length of each sentence\n",
        "#     tokens = tokens + ['[PAD]' for _ in range(maxlen-len(tokens))]\n",
        "#   else:\n",
        "#     tokens = tokens[:maxlen-1] + ['SEP']\n",
        "#   tokens_ids = tokenizer.convert_tokens_to_ids(tokens) # obtaining the indices of tokens in vocab\n",
        "#   tokens_ids_tensor = torch.tensor(tokens_ids).unsqueeze(0).cuda(gpu)\n",
        "\n",
        "#   attn_mask = (tokens_ids_tensor != 0).long().cuda(gpu)        # attention mask (identity where is padded)\n",
        "#   with torch.no_grad():\n",
        "#     prediction = bert_net2(tokens_ids_tensor, attn_mask)\n",
        "#   return prediction.view(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwTqkc7bfmYU"
      },
      "source": [
        "def predict3(model, X, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  source, replies = X\n",
        "  s_tokens = tokenizer.tokenize(source)\n",
        "  s_tokens = ['[CLS]'] + s_tokens + ['[SEP']       # insert CLS and SEP token\n",
        "  if len(s_tokens) < source_maxlen:                # keep the same length of each sentence\n",
        "    s_tokens = s_tokens + ['[PAD]' for _ in range(source_maxlen-len(s_tokens))]\n",
        "  else:\n",
        "    s_tokens = s_tokens[:source_maxlen-1] + ['SEP']\n",
        "\n",
        "  r_tokens = tokenizer.tokenize(replies)\n",
        "  r_tokens = ['[CLS]'] + r_tokens + ['[SEP']\n",
        "  if len(r_tokens) < reply_maxlen:                # keep the same length of each sentence\n",
        "    r_tokens = r_tokens + ['[PAD]' for _ in range(reply_maxlen-len(r_tokens))]\n",
        "  else:\n",
        "    r_tokens = r_tokens[:reply_maxlen-1] + ['SEP']\n",
        "\n",
        "  tokens_ids = tokenizer.convert_tokens_to_ids(s_tokens) + tokenizer.convert_tokens_to_ids(r_tokens) # obtaining the indices of tokens in vocab\n",
        "  tokens_ids_tensor = torch.tensor(tokens_ids).unsqueeze(0)  \n",
        "\n",
        "  attn_mask = (tokens_ids_tensor != 0).long()        # attention mask (identity where is padded)\n",
        "  \n",
        "  token_type_ids = torch.tensor([0 for _ in range(source_maxlen)]+[1 for _ in range(reply_maxlen)])\n",
        "  with torch.no_grad():\n",
        "    tokens_ids_tensor, attn_mask, token_type_ids = tokens_ids_tensor.cuda(gpu), attn_mask.cuda(gpu), token_type_ids.cuda(gpu)\n",
        "    prediction = model(tokens_ids_tensor, attn_mask, token_type_ids)\n",
        "  return prediction.view(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThRyXegGMMXZ",
        "outputId": "e5288998-446a-4139-e604-465b320cac5d"
      },
      "source": [
        "# preds_dev = [(1 if p >0.0 else 0) for p in [predict2(sent) for sent in dev_sources0]]\n",
        "\n",
        "print([predict2(dev_sources0[i]) for i in range(10)])\n",
        "\n",
        "\n",
        "[(1 if b>a else 0) for a,b in [predict2(dev_sources0[i]) for i in range(10)]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 1, 0, 0, 0, 0, 1, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EAdGXbTZM_t"
      },
      "source": [
        "# dev_preds = []\n",
        "\n",
        "# for i in range(len(dev_sources0)):\n",
        "#     print(i)\n",
        "#     p0, p1 = predict2(dev_sources0[i])\n",
        "#     if p1 > p0:\n",
        "#         dev_preds.append(1)\n",
        "#     else:\n",
        "#         dev_preds.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmmOpakeVJPZ"
      },
      "source": [
        "# test_preds = []\n",
        "\n",
        "# for i in range(len(dev_sources0)):\n",
        "#     print(i)\n",
        "#     p0, p1 = predict2(test_sources0[i])\n",
        "#     if p1 > p0:\n",
        "#         test_preds.append(1)\n",
        "#     else:\n",
        "#         test_preds.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO656j0xwJW3",
        "outputId": "75db2b43-a138-418d-c61c-1096bc31f207"
      },
      "source": [
        "preds_dict = dict(zip(test_ids, test_preds))\n",
        "\n",
        "with open(\"sample_data/bert.v2_test.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "storing file finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqXjj_eawa0o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRfVDRbt3MC"
      },
      "source": [
        "#### source + reply (bert)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0UDxAALt56P"
      },
      "source": [
        "path = \"drive/MyDrive/Colab/\"\n",
        "training_data = [json.loads(event) for event in open(path+'project-data/train.data.jsonl', \"r\").readlines()]\n",
        "dev_data = [json.loads(event) for event in open(path+'project-data/dev.data.jsonl', \"r\").readlines()]\n",
        "test_data = [json.loads(event) for event in open(path+'project-data/test.data.jsonl', \"r\").readlines()]\n",
        "\n",
        "train_labels = json.load(open(path+'project-data/train.label.json', \"r\"))\n",
        "dev_labels = json.load(open(path+'project-data/dev.label.json', \"r\"))\n",
        "\n",
        "train_labels = np.array([(1 if train_labels[id_str]=='rumour' else 0) for id_str in train_labels])\n",
        "dev_labels = np.array([(1 if dev_labels[id_str]=='rumour' else 0) for id_str in dev_labels])\n",
        "\n",
        "test_ids = [test_data[i][0][\"id_str\"] for i in range(len(test_data))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUIBVc7Ox6Mm"
      },
      "source": [
        "# sort by date for each event\n",
        "def to_date(date_str):\n",
        "    return datetime.strftime(datetime.strptime(date_str,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "training_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in training_data]\n",
        "dev_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in dev_data]\n",
        "test_data_sort = [sorted(event, key=lambda x : to_date(x[\"created_at\"])) for event in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIhxkg_ic-yT"
      },
      "source": [
        "# sort by user's followers_count\n",
        "training_data_sort1 = [training_data[0]] + [sorted(event, key=lambda x : x['user']['followers_count'], reverse=True) for event in training_data[1:]]\n",
        "dev_data_sort1 = [dev_data[0]] + [sorted(event, key=lambda x : x['user']['followers_count'], reverse=True) for event in dev_data[1:]]\n",
        "test_data_sort1 = [test_data[0]] + [sorted(event, key=lambda x : x['user']['followers_count'], reverse=True) for event in test_data[1:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFp6A3H4yAzk"
      },
      "source": [
        "# remove url\n",
        "def remove_urls(vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def remove_ats(vTEXT):\n",
        "    vTEXT = re.sub(r'@[^\\s]* ', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def extract_info(data, info=\"text\"):\n",
        "    res = []\n",
        "    for i in range(len(data)):\n",
        "        event = data[i]\n",
        "        event_info = []\n",
        "        for tw in event:\n",
        "            event_info.append(remove_ats(tw[info]))\n",
        "            # event_info.append(remove_ats(remove_urls(tw[info])))\n",
        "        res.append(event_info)\n",
        "    return res\n",
        "\n",
        "training_sents = extract_info(training_data_sort)     # {event}  where event={source,apply1,apply2,...}\n",
        "dev_sents = extract_info(dev_data_sort)\n",
        "test_sents = extract_info(test_data_sort)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6IMinYoydQm"
      },
      "source": [
        "def combine_replies(replies):\n",
        "    res = \"\"\n",
        "    for r in replies:\n",
        "        res += r\n",
        "    return res\n",
        "\n",
        "training_all0 = [[event[0], combine_replies(event[1:])] for event in training_sents]\n",
        "dev_all0 = [[event[0], combine_replies(event[1:])] for event in dev_sents]\n",
        "test_all0 = [[event[0], combine_replies(event[1:])] for event in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-BBxEerzCjK"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k26YPfZT0gwd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhmlXeJX0kfX"
      },
      "source": [
        "# define the dataset class\n",
        "class TwitrerDataset3(Dataset):\n",
        "  def __init__(self, X, y, source_maxlen, reply_maxlen):\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.source_maxlen = source_maxlen\n",
        "    self.reply_maxlen = reply_maxlen\n",
        "    self.X = X\n",
        "    self.y = y \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # selecting the sentence and label at the specific index\n",
        "    # sent = self.X[index]\n",
        "    source, replies = self.X[index]\n",
        "    label = self.y[index]\n",
        "\n",
        "    # preprocessing the text to be suitable for BERT\n",
        "    s_tokens = self.tokenizer.tokenize(source)\n",
        "    s_tokens = ['[CLS]'] + s_tokens + ['[SEP']       # insert CLS and SEP token\n",
        "    if len(s_tokens) < self.source_maxlen:                # keep the same length of each sentence\n",
        "      s_tokens = s_tokens + ['[PAD]' for _ in range(self.source_maxlen-len(s_tokens))]\n",
        "    else:\n",
        "      s_tokens = s_tokens[:self.source_maxlen-1] + ['SEP']\n",
        "\n",
        "    r_tokens = self.tokenizer.tokenize(replies)\n",
        "    r_tokens = r_tokens + ['[SEP']\n",
        "    if len(r_tokens) < self.reply_maxlen:                # keep the same length of each sentence\n",
        "      r_tokens = r_tokens + ['[PAD]' for _ in range(self.reply_maxlen-len(r_tokens))]\n",
        "    else:\n",
        "      r_tokens = r_tokens[:self.reply_maxlen-1] + ['SEP']\n",
        "\n",
        "\n",
        "    tokens_ids = self.tokenizer.convert_tokens_to_ids(s_tokens) + self.tokenizer.convert_tokens_to_ids(r_tokens) # obtaining the indices of tokens in vocab\n",
        "    tokens_ids_tensor = torch.tensor(tokens_ids)   \n",
        "\n",
        "    attn_mask = (tokens_ids_tensor != 0).long()        # attention mask (identity where is padded)\n",
        "\n",
        "    token_type_ids = torch.tensor([0 for _ in range(source_maxlen)]+[1 for _ in range(reply_maxlen)])\n",
        "    \n",
        "    return tokens_ids_tensor, attn_mask, token_type_ids, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWy8-hS2DRY"
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 64\n",
        "num_worders = 2\n",
        "lr = 2e-5\n",
        "source_maxlen, reply_maxlen = 30, 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "3c3bd62514d448208b80d21a0a4d44f6",
            "5c4b0f5024b04ebd84d10a07836a7dc5",
            "28e469072be84558b8dd70b7f1bcea13",
            "6b22f16374054640a4352381131c1a14",
            "75a53fd6377e4bc69c32b70f9979ba73",
            "310bfe08b58c45e89a0ec50daa4ff754",
            "06744300918c4a3ba93ec2d17469fab9",
            "40e5716f64b34811aba95631af26fb42",
            "51cecf60cd1544bba9a509cb65c941f8",
            "eeb372e4eedf42fd9fa7972b2ada05d3",
            "bb19a9dd33fd4e8fbf3876eb9960f39d",
            "f5d0972a2772468cb310f04a9f8608f9",
            "8327354d22a74826a8149e2a1b892491",
            "8c42bd70a4494f04a19943d9f7c7935f",
            "5927380634404d658f327a8c37f6a2a1",
            "b1bceaed595143d18c1b239c784732d3",
            "0c88f69e7a4142d9bfa2590cb2570da6",
            "b9301c5c50144f608cca3d251f180033",
            "81fc8b8e27af4dd8a57bf89f8ff9db64",
            "c0c813ab3d6e42ada30765291f79175a",
            "662329eaa04e442983e03294f42910bd",
            "3db866c38934445686b48a3b546a2142",
            "45cfc3092da540058cdafa883143c9da",
            "cab1ed9efcc64fab87f666c1365282b7"
          ]
        },
        "id": "p9Jw2QSQ4Bjr",
        "outputId": "1f51ab8b-330e-425c-9c0e-ad46b2d11e11"
      },
      "source": [
        "# creating instances of training and dev set\n",
        "train_set = TwitrerDataset3(training_all0, train_labels, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen)\n",
        "dev_set = TwitrerDataset3(dev_all0, dev_labels, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen)\n",
        "\n",
        "# creating dataset loader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_worders)\n",
        "dev_loader = DataLoader(dev_set, batch_size=batch_size, num_workers=num_worders)\n",
        "\n",
        "print(\"Done preprocessing training and development data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c3bd62514d448208b80d21a0a4d44f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51cecf60cd1544bba9a509cb65c941f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c88f69e7a4142d9bfa2590cb2570da6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done preprocessing training and development data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kho9Dhhg4ERF"
      },
      "source": [
        "class RumourClassifier3(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(RumourClassifier3, self).__init__()\n",
        "    self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.cls_layer = nn.Linear(768, 1)\n",
        "    self.dropout = nn.Dropout(0.9)\n",
        "\n",
        "  def forward(self, seq, attn_masks, token_type_ids):\n",
        "    '''\n",
        "    Inputs:\n",
        "      -seq: Tensor of shape [B, T] containing token ids of sequences\n",
        "      -attn_masks: Tensor of shape [B, T] containing attention masks to be used\n",
        "    '''\n",
        "    # feed the input to bert model to obtain contextualized representation\n",
        "    outputs = self.bert_layer(seq, attention_mask=attn_masks, token_type_ids=token_type_ids)\n",
        "    cont_reps = outputs.last_hidden_state\n",
        "\n",
        "    # obtaining the representation of [CLS] head\n",
        "    cls_rep = cont_reps[:, 0]\n",
        "\n",
        "    # pooled_output = self.dropout(cls_rep)\n",
        "\n",
        "    # feeding cls_rep into the classifier layer\n",
        "    logits = self.cls_layer(cls_rep)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g38mwWwIKH1P"
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# def get_precision_from_logits(logits, labels):\n",
        "#     probs = torch.sigmoid(logits)\n",
        "#     soft_probs = (probs > 0.5).long()\n",
        "#     labels_cpu = labels.cpu()\n",
        "#     soft_probs_cpu = soft_probs.cpu()\n",
        "#     return precision_recall_fscore_support(labels_cpu, soft_probs_cpu)[0][1]\n",
        "\n",
        "\n",
        "# def get_recall_from_logits(logits, labels):\n",
        "#     probs = torch.sigmoid(logits)\n",
        "#     soft_probs = (probs > 0.5).long()\n",
        "#     labels_cpu = labels.cpu()\n",
        "#     soft_probs_cpu = soft_probs.cpu()\n",
        "#     return precision_recall_fscore_support(labels_cpu, soft_probs_cpu)[1][1]\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "    mean_acc, mean_loss, mean_precision, mean_recall = 0, 0, 0, 0\n",
        "    count = 0\n",
        "    tn, fp, fn, tp = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, token_type_ids, labels in dataloader:\n",
        "            bs = labels.shape[0]\n",
        "            seq, attn_masks, token_type_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), token_type_ids.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()*bs\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)*bs\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            soft_probs = (probs > 0.5).long()\n",
        "            labels_cpu, soft_probs_cpu = labels.cpu(), soft_probs.cpu()\n",
        "            tn_, fp_, fn_, tp_ = confusion_matrix(labels_cpu, soft_probs_cpu).ravel()\n",
        "            tn += tn_\n",
        "            fp += fp_\n",
        "            fn += fn_\n",
        "            tp += tp_\n",
        "            count += bs\n",
        "\n",
        "    return mean_acc / count, tp / (tp+fp), tp / (tp+fn), mean_loss / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNKYYlqZJjAB"
      },
      "source": [
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_f1 = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        print()\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), token_type_ids.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; acc: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc,(time.time()-st)))\n",
        "                st = time.time()\n",
        "        \n",
        "        dev_acc, dev_precision, dev_recall,dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        dev_f1 = (2*dev_precision*dev_recall/(dev_precision+dev_recall))\n",
        "        print(\"*****Epoch {} complete! Development f1-score: {}; Development Precision: {}; Development Recall: {}; Development Loss: {}\".format(ep, dev_f1, dev_precision, dev_recall, dev_loss))\n",
        "        if dev_f1 > best_f1:\n",
        "            print(\"Best development f1-score improved from {} to {}, saving model...\".format(best_f1, dev_f1))\n",
        "            best_f1 = dev_f1\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "fec19e40cd2f44e782a66b90b01713d8",
            "15dab6be997f48468040d9cd8812fa85",
            "6adfaef5046a436f82de90841623c603",
            "521892d0f54640d99d4f49f62d8bf150",
            "4dcb38aa644242d280f871c3db44680b",
            "cf8dc33780d0403bb07fd0659499f069",
            "581928a99d3c43f7ab32c332fb196620",
            "d55dfdf4de10471ebe8ffa65cf32d422",
            "653b05b264184685b62b7b2a1ef3122b",
            "0e1aeddabf424accae7768aba4b3846e",
            "66237b19f48f42e98c22fd55dbd444c1",
            "5889dfb8f14f4a05bd681e1da58608bd",
            "9c5e30900123403aa0035c9bcf3948d4",
            "333c00b9eb3e493b8cc83a507c7e132b",
            "194204aac10648a184195d3d395df030",
            "77ffa593bef84e9fb5ac84879fb45427"
          ]
        },
        "id": "rEqdSGmi46HG",
        "outputId": "540c1d2c-f24c-456b-f5ec-46490a403991"
      },
      "source": [
        "gpu = 0  # GPU id\n",
        "\n",
        "bert_net3 = RumourClassifier3()\n",
        "bert_net3.cuda(gpu) \n",
        "print(\"creating the rumour classifier: Done!\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fec19e40cd2f44e782a66b90b01713d8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "653b05b264184685b62b7b2a1ef3122b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "creating the rumour classifier: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCimw9oO2Fc6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxnxktYTFZhs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRpmd3SQFZnb",
        "outputId": "18de4eb3-72f9-436d-c177-bedf8a178212"
      },
      "source": [
        "# sort by date, reply_max = 30, pool, just remove ats\n",
        "num_epoch = 8\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# opti = optim.Adam(bert_net3.parameters(), lr=lr)\n",
        "opti = AdamW(bert_net3.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net3, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.6819823980331421; acc: 0.59375; Time taken (s): 1.6413025856018066\n",
            "*****Epoch 0 complete! Development f1-score: 0.7806122448979593; Development Precision: 0.7463414634146341; Development Recall: 0.8181818181818182; Development Loss: 0.34832867055103695\n",
            "Best development f1-score improved from 0 to 0.7806122448979593, saving model...\n",
            "\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.3537675738334656; acc: 0.8125; Time taken (s): 48.426135778427124\n",
            "*****Epoch 1 complete! Development f1-score: 0.7701149425287356; Development Precision: 0.8322981366459627; Development Recall: 0.7165775401069518; Development Loss: 0.32124878332532686\n",
            "\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.22718462347984314; acc: 0.890625; Time taken (s): 49.46395969390869\n",
            "*****Epoch 2 complete! Development f1-score: 0.8048192771084337; Development Precision: 0.7324561403508771; Development Recall: 0.893048128342246; Development Loss: 0.3357803616030463\n",
            "Best development f1-score improved from 0.7806122448979593 to 0.8048192771084337, saving model...\n",
            "\n",
            "Iteration 0 of epoch 3 complete. Loss: 0.1742950677871704; acc: 0.9375; Time taken (s): 53.92014813423157\n",
            "*****Epoch 3 complete! Development f1-score: 0.8189415041782729; Development Precision: 0.8546511627906976; Development Recall: 0.786096256684492; Development Loss: 0.35421948268495757\n",
            "Best development f1-score improved from 0.8048192771084337 to 0.8189415041782729, saving model...\n",
            "\n",
            "Iteration 0 of epoch 4 complete. Loss: 0.04167083278298378; acc: 0.984375; Time taken (s): 53.2841432094574\n",
            "*****Epoch 4 complete! Development f1-score: 0.8306878306878307; Development Precision: 0.8219895287958116; Development Recall: 0.839572192513369; Development Loss: 0.42821776949126145\n",
            "Best development f1-score improved from 0.8189415041782729 to 0.8306878306878307, saving model...\n",
            "\n",
            "Iteration 0 of epoch 5 complete. Loss: 0.008432960137724876; acc: 1.0; Time taken (s): 53.352412939071655\n",
            "*****Epoch 5 complete! Development f1-score: 0.8164383561643836; Development Precision: 0.8370786516853933; Development Recall: 0.7967914438502673; Development Loss: 0.49109998900314855\n",
            "\n",
            "Iteration 0 of epoch 6 complete. Loss: 0.0017459073569625616; acc: 1.0; Time taken (s): 52.06405711174011\n",
            "*****Epoch 6 complete! Development f1-score: 0.8232044198895027; Development Precision: 0.8514285714285714; Development Recall: 0.7967914438502673; Development Loss: 0.49926149105203566\n",
            "\n",
            "Iteration 0 of epoch 7 complete. Loss: 0.0011620573932304978; acc: 1.0; Time taken (s): 51.88633489608765\n",
            "*****Epoch 7 complete! Development f1-score: 0.8176795580110499; Development Precision: 0.8457142857142858; Development Recall: 0.7914438502673797; Development Loss: 0.5531535880319003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGLDzsJ32Flb",
        "outputId": "a36d2936-b408-4a17-ca02-70c257abec67"
      },
      "source": [
        "# sort by date, reply_max = 50\n",
        "num_epoch = 10\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# opti = optim.Adam(bert_net3.parameters(), lr=lr)\n",
        "opti = AdamW(bert_net3.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net3, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.7804609537124634; acc: 0.375; Time taken (s): 1.801375389099121\n",
            "*****Epoch 0 complete! Development f1-score: 0.7788944723618091; Development Precision: 0.7345971563981043; Development Recall: 0.8288770053475936; Development Loss: 0.34324746707390097\n",
            "Best development f1-score improved from 0 to 0.7788944723618091, saving model...\n",
            "\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.36333340406417847; acc: 0.8125; Time taken (s): 74.36376881599426\n",
            "*****Epoch 1 complete! Development f1-score: 0.7851002865329513; Development Precision: 0.845679012345679; Development Recall: 0.732620320855615; Development Loss: 0.3160153450637028\n",
            "Best development f1-score improved from 0.7788944723618091 to 0.7851002865329513, saving model...\n",
            "\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.22815145552158356; acc: 0.875; Time taken (s): 74.3490617275238\n",
            "*****Epoch 2 complete! Development f1-score: 0.8040712468193385; Development Precision: 0.7669902912621359; Development Recall: 0.8449197860962567; Development Loss: 0.313158885569408\n",
            "Best development f1-score improved from 0.7851002865329513 to 0.8040712468193385, saving model...\n",
            "\n",
            "Iteration 0 of epoch 3 complete. Loss: 0.09667479991912842; acc: 0.9375; Time taken (s): 74.05327916145325\n",
            "*****Epoch 3 complete! Development f1-score: 0.8189415041782729; Development Precision: 0.8546511627906976; Development Recall: 0.786096256684492; Development Loss: 0.38309555053710936\n",
            "Best development f1-score improved from 0.8040712468193385 to 0.8189415041782729, saving model...\n",
            "\n",
            "Iteration 0 of epoch 4 complete. Loss: 0.011567267589271069; acc: 1.0; Time taken (s): 73.82789897918701\n",
            "*****Epoch 4 complete! Development f1-score: 0.8157248157248157; Development Precision: 0.7545454545454545; Development Recall: 0.8877005347593583; Development Loss: 0.43552508395293665\n",
            "\n",
            "Iteration 0 of epoch 5 complete. Loss: 0.009726108983159065; acc: 1.0; Time taken (s): 72.43542098999023\n",
            "*****Epoch 5 complete! Development f1-score: 0.7833827893175075; Development Precision: 0.88; Development Recall: 0.7058823529411765; Development Loss: 0.5470356234188738\n",
            "\n",
            "Iteration 0 of epoch 6 complete. Loss: 0.00407909881323576; acc: 1.0; Time taken (s): 72.17703366279602\n",
            "*****Epoch 6 complete! Development f1-score: 0.8069164265129684; Development Precision: 0.875; Development Recall: 0.7486631016042781; Development Loss: 0.5725570884244195\n",
            "\n",
            "Iteration 0 of epoch 7 complete. Loss: 0.0009133614948950708; acc: 1.0; Time taken (s): 72.15665364265442\n",
            "*****Epoch 7 complete! Development f1-score: 0.8111111111111111; Development Precision: 0.8439306358381503; Development Recall: 0.7807486631016043; Development Loss: 0.566868598707791\n",
            "\n",
            "Iteration 0 of epoch 8 complete. Loss: 0.0005107277538627386; acc: 1.0; Time taken (s): 72.02163887023926\n",
            "*****Epoch 8 complete! Development f1-score: 0.8133704735376045; Development Precision: 0.8488372093023255; Development Recall: 0.7807486631016043; Development Loss: 0.5981719814497849\n",
            "\n",
            "Iteration 0 of epoch 9 complete. Loss: 0.0003956499567721039; acc: 1.0; Time taken (s): 72.10222220420837\n",
            "*****Epoch 9 complete! Development f1-score: 0.8067226890756303; Development Precision: 0.8470588235294118; Development Recall: 0.7700534759358288; Development Loss: 0.6214391601496729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXlin58t2Ep2",
        "outputId": "5d387229-614d-49e1-8141-6c9b90abbd00"
      },
      "source": [
        "# sort by date, reply_max = 100\n",
        "num_epoch = 10\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(bert_net3.parameters(), lr=lr)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net3, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.6802589893341064; acc: 0.640625; Time taken (s): 2.4174270629882812\n",
            "*****Epoch 0 complete! Development f1-score: 0.772020725388601; Development Precision: 0.7487437185929648; Development Recall: 0.7967914438502673; Development Loss: 0.3357150692364265\n",
            "Best development f1-score improved from 0 to 0.772020725388601, saving model...\n",
            "\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.3574814796447754; acc: 0.828125; Time taken (s): 120.77228260040283\n",
            "*****Epoch 1 complete! Development f1-score: 0.7684964200477327; Development Precision: 0.6939655172413793; Development Recall: 0.8609625668449198; Development Loss: 0.3547134128110162\n",
            "\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.18147578835487366; acc: 0.921875; Time taken (s): 118.39115238189697\n",
            "*****Epoch 2 complete! Development f1-score: 0.7133757961783439; Development Precision: 0.8818897637795275; Development Recall: 0.5989304812834224; Development Loss: 0.4090502911600573\n",
            "\n",
            "Iteration 0 of epoch 3 complete. Loss: 0.23333245515823364; acc: 0.875; Time taken (s): 118.76065421104431\n",
            "*****Epoch 3 complete! Development f1-score: 0.8040201005025125; Development Precision: 0.7582938388625592; Development Recall: 0.8556149732620321; Development Loss: 0.38710880855034135\n",
            "Best development f1-score improved from 0.772020725388601 to 0.8040201005025125, saving model...\n",
            "\n",
            "Iteration 0 of epoch 4 complete. Loss: 0.05456036701798439; acc: 0.96875; Time taken (s): 120.00878238677979\n",
            "*****Epoch 4 complete! Development f1-score: 0.8100558659217877; Development Precision: 0.847953216374269; Development Recall: 0.7754010695187166; Development Loss: 0.433052816062138\n",
            "Best development f1-score improved from 0.8040201005025125 to 0.8100558659217877, saving model...\n",
            "\n",
            "Iteration 0 of epoch 5 complete. Loss: 0.0075807953253388405; acc: 1.0; Time taken (s): 119.90098643302917\n",
            "*****Epoch 5 complete! Development f1-score: 0.8112676056338027; Development Precision: 0.8571428571428571; Development Recall: 0.7700534759358288; Development Loss: 0.49341834413594216\n",
            "Best development f1-score improved from 0.8100558659217877 to 0.8112676056338027, saving model...\n",
            "\n",
            "Iteration 0 of epoch 6 complete. Loss: 0.0014267020160332322; acc: 1.0; Time taken (s): 119.78485417366028\n",
            "*****Epoch 6 complete! Development f1-score: 0.8156424581005587; Development Precision: 0.8538011695906432; Development Recall: 0.7807486631016043; Development Loss: 0.5249206378542144\n",
            "Best development f1-score improved from 0.8112676056338027 to 0.8156424581005587, saving model...\n",
            "\n",
            "Iteration 0 of epoch 7 complete. Loss: 0.0008358840132132173; acc: 1.0; Time taken (s): 119.64834690093994\n",
            "*****Epoch 7 complete! Development f1-score: 0.8146067415730336; Development Precision: 0.8579881656804734; Development Recall: 0.7754010695187166; Development Loss: 0.5504115474635157\n",
            "\n",
            "Iteration 0 of epoch 8 complete. Loss: 0.0005932670319452882; acc: 1.0; Time taken (s): 118.35075211524963\n",
            "*****Epoch 8 complete! Development f1-score: 0.8179271708683472; Development Precision: 0.8588235294117647; Development Recall: 0.7807486631016043; Development Loss: 0.5710986342923394\n",
            "Best development f1-score improved from 0.8156424581005587 to 0.8179271708683472, saving model...\n",
            "\n",
            "Iteration 0 of epoch 9 complete. Loss: 0.00046108680544421077; acc: 1.0; Time taken (s): 119.6333339214325\n",
            "*****Epoch 9 complete! Development f1-score: 0.8179271708683472; Development Precision: 0.8588235294117647; Development Recall: 0.7807486631016043; Development Loss: 0.5889119699083526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB7HS1wJjlXQ",
        "outputId": "1bb4c66d-11f4-4069-ed7e-eb37ac08fd05"
      },
      "source": [
        "# sort by followers\n",
        "num_epoch = 10\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(bert_net3.parameters(), lr=lr)\n",
        "\n",
        "# fine-tune the bert network\n",
        "train(bert_net3, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.6820484399795532; f1: 0.625; Precision: 0.5; Recall: 0.2916666666666667; Time taken (s): 1.787294626235962\n",
            "*****Epoch 0 complete! Development f1-score: 0.7298850574712643; Development Precision: 0.7888198757763976; Development Recall: 0.679144385026738; Development Loss: 0.3668898031629365\n",
            "Best development f1-score improved from 0 to 0.7298850574712643, saving model...\n",
            "\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.3754526972770691; f1: 0.828125; Precision: 0.782608695652174; Recall: 0.75; Time taken (s): 74.44451594352722\n",
            "*****Epoch 1 complete! Development f1-score: 0.7783505154639175; Development Precision: 0.7512437810945274; Development Recall: 0.8074866310160428; Development Loss: 0.34727517983009076\n",
            "Best development f1-score improved from 0.7298850574712643 to 0.7783505154639175, saving model...\n",
            "\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.2469254434108734; f1: 0.875; Precision: 0.7857142857142857; Recall: 0.9166666666666666; Time taken (s): 74.52195191383362\n",
            "*****Epoch 2 complete! Development f1-score: 0.7219917012448133; Development Precision: 0.5898305084745763; Development Recall: 0.93048128342246; Development Loss: 0.5531217266773355\n",
            "\n",
            "Iteration 0 of epoch 3 complete. Loss: 0.31437021493911743; f1: 0.84375; Precision: 0.7058823529411765; Recall: 1.0; Time taken (s): 72.38038873672485\n",
            "*****Epoch 3 complete! Development f1-score: 0.7746478873239435; Development Precision: 0.6903765690376569; Development Recall: 0.8823529411764706; Development Loss: 0.45080204832142795\n",
            "\n",
            "Iteration 0 of epoch 4 complete. Loss: 0.07851293683052063; f1: 0.984375; Precision: 0.96; Recall: 1.0; Time taken (s): 72.29112005233765\n",
            "*****Epoch 4 complete! Development f1-score: 0.7756232686980609; Development Precision: 0.8045977011494253; Development Recall: 0.7486631016042781; Development Loss: 0.5144470371049026\n",
            "\n",
            "Iteration 0 of epoch 5 complete. Loss: 0.12780781090259552; f1: 0.953125; Precision: 0.92; Recall: 0.9583333333333334; Time taken (s): 71.95041823387146\n",
            "*****Epoch 5 complete! Development f1-score: 0.7722222222222223; Development Precision: 0.8034682080924855; Development Recall: 0.7433155080213903; Development Loss: 0.575310284515907\n",
            "\n",
            "Iteration 0 of epoch 6 complete. Loss: 0.011815151199698448; f1: 1.0; Precision: 1.0; Recall: 1.0; Time taken (s): 71.96681499481201\n",
            "*****Epoch 6 complete! Development f1-score: 0.7784090909090908; Development Precision: 0.8303030303030303; Development Recall: 0.732620320855615; Development Loss: 0.6242102951839053\n",
            "Best development f1-score improved from 0.7783505154639175 to 0.7784090909090908, saving model...\n",
            "\n",
            "Iteration 0 of epoch 7 complete. Loss: 0.0024692905135452747; f1: 1.0; Precision: 1.0; Recall: 1.0; Time taken (s): 73.49437594413757\n",
            "*****Epoch 7 complete! Development f1-score: 0.7777777777777777; Development Precision: 0.8092485549132948; Development Recall: 0.7486631016042781; Development Loss: 0.6803828272326239\n",
            "\n",
            "Iteration 0 of epoch 8 complete. Loss: 0.0011705923825502396; f1: 1.0; Precision: 1.0; Recall: 1.0; Time taken (s): 71.97532534599304\n",
            "*****Epoch 8 complete! Development f1-score: 0.7777777777777777; Development Precision: 0.8092485549132948; Development Recall: 0.7486631016042781; Development Loss: 0.7111449110096899\n",
            "\n",
            "Iteration 0 of epoch 9 complete. Loss: 0.0008301082998514175; f1: 1.0; Precision: 1.0; Recall: 1.0; Time taken (s): 71.97564840316772\n",
            "*****Epoch 9 complete! Development f1-score: 0.7823691460055097; Development Precision: 0.8068181818181818; Development Recall: 0.7593582887700535; Development Loss: 0.7369164417529929\n",
            "Best development f1-score improved from 0.7784090909090908 to 0.7823691460055097, saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6yKsjwgKs8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8qJEoK0KtCD"
      },
      "source": [
        "def predict3(model, X, source_maxlen=source_maxlen, reply_maxlen=reply_maxlen):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  source, replies = X\n",
        "  s_tokens = tokenizer.tokenize(source)\n",
        "  s_tokens = ['[CLS]'] + s_tokens + ['[SEP']       # insert CLS and SEP token\n",
        "  if len(s_tokens) < source_maxlen:                # keep the same length of each sentence\n",
        "    s_tokens = s_tokens + ['[PAD]' for _ in range(source_maxlen-len(s_tokens))]\n",
        "  else:\n",
        "    s_tokens = s_tokens[:source_maxlen-1] + ['SEP']\n",
        "\n",
        "  r_tokens = tokenizer.tokenize(replies)\n",
        "  r_tokens = ['[CLS]'] + r_tokens + ['[SEP']\n",
        "  if len(r_tokens) < reply_maxlen:                # keep the same length of each sentence\n",
        "    r_tokens = r_tokens + ['[PAD]' for _ in range(reply_maxlen-len(r_tokens))]\n",
        "  else:\n",
        "    r_tokens = r_tokens[:reply_maxlen-1] + ['SEP']\n",
        "\n",
        "  tokens_ids = tokenizer.convert_tokens_to_ids(s_tokens) + tokenizer.convert_tokens_to_ids(r_tokens) # obtaining the indices of tokens in vocab\n",
        "  tokens_ids_tensor = torch.tensor(tokens_ids).unsqueeze(0)  \n",
        "\n",
        "  attn_mask = (tokens_ids_tensor != 0).long()        # attention mask (identity where is padded)\n",
        "  \n",
        "  token_type_ids = torch.tensor([0 for _ in range(source_maxlen)]+[1 for _ in range(reply_maxlen)])\n",
        "  with torch.no_grad():\n",
        "    tokens_ids_tensor, attn_mask, token_type_ids = tokens_ids_tensor.cuda(gpu), attn_mask.cuda(gpu), token_type_ids.cuda(gpu)\n",
        "    prediction = model(tokens_ids_tensor, attn_mask, token_type_ids)\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QEArdlI57nO"
      },
      "source": [
        "# # preds_dev = [(1 if p >0.0 else 0) for p in [predict3(bert_net3, source_reply) for source_reply in dev_all0]]\n",
        "# preds_dev = []\n",
        "# for i in range(len(dev_all0)):\n",
        "#   if i%100==0:\n",
        "#     print(i)\n",
        "#   pred = predict3(bert_net3, dev_all0[i])\n",
        "#   pred_label = 1 if pred>0 else 0\n",
        "#   preds_dev.append(pred_label)\n",
        "\n",
        "# precision_recall_fscore_support(preds_dev, dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKjIN8HKWnbD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhVmA5CDWnfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34dc415-3f22-4768-fe97-b6feaa514df8"
      },
      "source": [
        "evaluate(bert_net3, criterion, dev_loader, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8862, device='cuda:0'),\n",
              " 0.8457142857142858,\n",
              " 0.7914438502673797,\n",
              " 0.5531535880319003)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jp6zkQHF8-X",
        "outputId": "213af48b-87e6-4a6b-e0cb-8456c96d530c"
      },
      "source": [
        "bert_net3.load_state_dict(torch.load(\"sstcls_4.dat\"))\n",
        "evaluate(bert_net3, criterion, dev_loader, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8897, device='cuda:0'),\n",
              " 0.8219895287958116,\n",
              " 0.839572192513369,\n",
              " 0.42821776949126145)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O2RcftsGjlD",
        "outputId": "3bf64ee9-4b4b-4ba4-b0f4-32f2d82f29ab"
      },
      "source": [
        "preds_dev = [(1 if p >0.0 else 0) for p in [predict3(bert_net3, source_reply) for source_reply in dev_all0]]\n",
        "precision_recall_fscore_support(preds_dev, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.92875318, 0.81818182]),\n",
              " array([0.91478697, 0.84530387]),\n",
              " array([0.92171717, 0.83152174]),\n",
              " array([399, 181]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zqvDB4CGmU5",
        "outputId": "9d937bc7-9b07-4178-a4d9-921c0e488125"
      },
      "source": [
        "evaluate(bert_net3, criterion, dev_loader, gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8897, device='cuda:0'),\n",
              " 0.8219895287958116,\n",
              " 0.839572192513369,\n",
              " 0.42821776949126145)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gz-r33QSzFO",
        "outputId": "f941af3b-3574-47e7-cd5c-038c04bafd88"
      },
      "source": [
        "def f1(p, r):\n",
        "  return 2*p*r/(p+r)\n",
        "\n",
        "print(f1(0.8219895287958116,0.839572192513369,))\n",
        "\n",
        "# date, 30 30, only remove ats: 0.83069\n",
        "# date 50: 0.81517\n",
        "# date 100: 0. 8179\n",
        "# date 50 adam: 0.8189415"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8306878306878307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMd-UUv7LqP5",
        "outputId": "1e83f04b-45f6-45ef-dfe7-d913dbd8410a"
      },
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uBX0HtEOSRM"
      },
      "source": [
        "predictions = [predict3(bert_net3, sent) for sent in test_all0]\n",
        "preds1 = [(\"rumour\" if pred > 0.0 else \"non-rumour\") for pred in predictions]\n",
        "preds_dict = dict(zip(test_ids, preds1))\n",
        "\n",
        "with open(\"sample_data/bert.v1_test_adamw_30_sourcereply.predict.json\",\"w\") as f:\n",
        "    json.dump(preds_dict,f)\n",
        "    print(\"storing file finish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOZw1idtnTFc",
        "outputId": "749531ad-d765-43b4-d7d9-99161e7d1fad"
      },
      "source": [
        "preds1 = [(1 if pred > 0.0 else 0) for pred in predictions]\n",
        "precision_recall_fscore_support(dev_labels, preds1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.91478697, 0.84530387]),\n",
              " array([0.92875318, 0.81818182]),\n",
              " array([0.92171717, 0.83152174]),\n",
              " array([393, 187]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}